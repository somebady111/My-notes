work_list

- 4-23
  - In the morning:
    - [x] 1.Query mqtt related parameters 
    - [x] 2.Testing program threads and processes
  
  - In the afternoon:
    - [x] Scrapy framework learning
  
- 4-24
  - In the morning:
    - [x] 1.Learning Network Programming
    - [x] 2.Code Logic modification
    - [x] 3.Sort and summarzing
  - In the afternoon:
    - [x] 1.Scrapy framework learning
  
- 4-25
  - In the morning:
    - [x] 1.Test My Program
    - [x] 2.Learning Network Programming
  
- 4-26
  - In the morning:
    - [x] 1.Scrapy framework learning
    - [x] 2.Test my program
    - [ ] 3.Based on the previous example,I crawled a recruitment website for myself.
  
- 4-27
  - What needs to be done：
    - [ ] Write two crawler programs a week and follow scrapy requirements
  - crawler mission
    - [ ] Agent pool
    - [ ] Reconstruction of Pipeline Function
    - [ ] scrapy docking selenium
    - [ ] scrapy docking splash
    - [ ] Scrapy universal crawler -- 中华科技网
    - [ ] scrapy docking docker
    - [ ] Distributed Reptiles
  
- 4-28
  - Long-term learning plan
    - [ ] web
      - [ ] django+flask+tornado+vue.js+node.js
      - [ ] bootstra
    - [ ] 数据分析
    - [ ] 爬虫
    - [ ] 机器学习
  
- 5-6~5-11

  |           本周计划           |       计划进度        |                             备注                             |
  | :--------------------------: | :-------------------: | :----------------------------------------------------------: |
  | 两个爬虫项目1.美团2.智联招聘 |         暂停          | 1.美团爬取产生页面跳转2.智联页面跳转登录页面，方法添加请求头的登录信息 |
  |      数据分析可视化学习      |                       |                                                              |
  |         修改代码bug          | 1.完成curdata表插入2. |                     1.插入一定要注意字段                     |

- 5-15—5-18

  |              本周计划              | 计划进度 |                备注                 |
  | :--------------------------------: | :------: | :---------------------------------: |
  |             知识点回归             |          | 1.爬虫起步+数据分析2.小周视频做预备 |
  |              项目进度              |          |                                     |
  |               Django               |          |                                     |
| 爬虫项目：1.爱美时尚网2.东方财富网 |  完成1   |                                     |
  
- 5-19—5-30

  |       计划        | 进度 |                     备注                     |
  | :---------------: | :--: | :------------------------------------------: |
  | numpy和Django结合 |      |                 1.5-25的视频                 |
  | 机器学习和pandas  |      | 1.理解为先2.总结其次3.栗子在后4.拓展最终检验 |
  |  连接的预备方案   |      |                   1.socket                   |
|     上线部署      |      |             1.配备一个Linux环境              |
  
- 7-16

|            待完成的            |            需要了解的             |                      需要加强理解练习的                      |
| :----------------------------: | :-------------------------------: | :----------------------------------------------------------: |
|            typeidea            |               算法                |                    python基础、数据库基础                    |
|           部署和镜像           |            前后端分离             |                     对类的理解和多写脚本                     |
|           mitmproxy            | sscrapy框架、django框架的内部实现 | nnumpy、pandas、机器学习、深度学习多练习和对一些分类方法和代码的结合多理解 |
| scrapy分布式、线程池、pyspider |                                   |                                                              |
|                                |                                   |                                                              |
|                                |                                   |                                                              |

- 7-16 next-test

| 标签 |     名称     |                       备注                       |
| :--: | :----------: | :----------------------------------------------: |
| 爬虫 | csdn_app爬取 | 1.下午把信息写入文本或者表格2.进行分析3.投递简历 |
|      |              |                                                  |

- 8-31

| 学习、工作内容 |      完成度      | 是否有笔记 | 时间     |            备注             |
| :------------: | :--------------: | :--------: | -------- | :-------------------------: |
|   kail Linux   | 1.安装2.工具使用 |     有     | 2019-9-2 |    下阶段：基础工具使用     |
|    投递简历    |                  |            |          |        8：30  13：30        |
|      习惯      |   19-10-1开始    |            |          | 早8：30-12：30下1：30-6：30 |

- 9-3
	- 1.爬虫和数据分析
	- 2.渗透测试
- 12-6
	- 1.kali系统学习
	- 2.爬虫工具项目化使用
	- 3.打包、分发、docker的使用总结出一套教程出来
	- 4.关注安全网消息
	- 5.开发模块结构调整
- 12-20（第一月钱花费计划）
	- 1.无线网卡（kali版本）
	- 2.域名（www.quicksand.xxx）
	- 3.买一Linux相关书
- 2020-1-19
	- 1.myworld模板构思
	- 2.数学
- 2020-1-25
	- 1.倒数的悖论
- 2020-1-29
	- plan1：完成myworld，购买域名上
- 2020-9-20
	- plan1：复习Linux知识点
		- 1.虚拟环境搭建
		- 2.基础操作命令
		- 3.kali更新命令
	- plan2：网络编程复习
		- 1.

